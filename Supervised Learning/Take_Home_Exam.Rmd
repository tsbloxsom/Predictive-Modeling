---
title: "Predictive_Models_Exam_Truett_Bloxsom"
output:
  pdf_document: default
  html_document: default
---
Chapter 2 Question 10
```{r}
library(MASS)
dim(Boston)
attach(Boston)
#?Boston
```
(a) there are 506 rows and 14 columns. Each row represents data on a particular suburb, and the columns represent data for one variable for all the suburbs.
```{r}
pairs(~., data = Boston)
```
(b) there seems to be a few pairs of variables that are associated with eachother like lstat and medv or dis and nox
```{r}
plot(dis, crim)
plot(medv, crim)
```
(c) There seems to be a relationship between crime rate and distance from epmloyments centers with crime increasing as the distance to the employments centers decreases there is also a relationship between crime rate and median house value with crime decreasing as the median house value increases.
```{r}
plot(crim, xlab = "suburb")
plot(tax, xlab = "suburb")
plot(ptratio, xlab = "suburb")
```
(d) the suburbs indexed from around 375 to 475 seem to have higher crime rates than the rest of the dataset. For these same suburbs, the tax rate is high and the pupil-teacher ratio is also high. the crime rate ranges from 0 to 100. The tax rate ranges from 0 to 700+. the pupil-teacher raion ranges from 12 to 22.
```{r}
length(which(chas == 1))
```
(e) 35 suburbs bound the charles river
```{r}
mean(ptratio)
```
(f) the mean pupil-teacher ratio is 18.46
```{r}
min(Boston[,14])
plot(medv)
Boston[406,]
summary(Boston)
```
(g) the 406th suburb has the lowest median value of owner-occupied homes with a value of 5. The Boston[406,] output summaizes the values of the other predictors. The crime in suburb 406, 67, is much higher than the average, 3.6.  406 has more non-retial busniess, 18.1, than the average at 11.1. 406 has a higher nitrogen oxides consentration, .69, than the avergae at .55. 406 has fewer rooms, 5.6, than the average at 6.2. 406 has a higher age of housing, 100, than the average at 68. 406 is closer to employment centers, 1.4, than the average at 3.8. 406 is much farther away from highways, 24, than the average at 9.5. 406 has much higher tax rate, 666, than the average at 408. 406 has a higher pupil-teacher ratio, 20.2, than the average at 18.46 406 has a higher porportion of blacks, 385, than the average at 357. 406 has a much higher lower status, 23, than the average at 13. The 406th suburb seems to be a low-income/poor neighborhood with high pollution, high crime, and older population compered to the average suburb in boston. 
```{r}
sum(rm > 7)
sum(rm > 8)
library(dplyr)
rm8 = filter(Boston, rm > 8)
summary(rm8)
summary(Boston)
```
(h) 64 suburbs average more than 7 rooms per dwelling and 13 average more than 8 rooms per dwelling. The dwellings that average more than 8 rooms per dwelling have lower crime rates, lower non-retial, higher age, lower distance to highways, lower property tax rates, lower pupil-teacher ratio, higher porportion of blacks, lower lower status of the population, and higher median value of homes than the average suburb.

Chapter 3 Question 15
```{r}
lm.fit = lm(crim~zn, data = Boston)
summary(lm.fit)
lm.fit1 = lm(crim~indus, data = Boston)
summary(lm.fit1)
lm.fit2 = lm(crim~chas, data = Boston)
summary(lm.fit2)
lm.fit3 = lm(crim~nox, data = Boston)
summary(lm.fit3)
lm.fit4 = lm(crim~rm, data = Boston)
summary(lm.fit4)
lm.fit5 = lm(crim~age, data = Boston)
summary(lm.fit5)
lm.fit6 = lm(crim~dis, data = Boston)
summary(lm.fit6)
lm.fit7 = lm(crim~rad, data = Boston)
summary(lm.fit7)
lm.fit8 = lm(crim~tax, data = Boston)
summary(lm.fit8)
lm.fit9 = lm(crim~ptratio, data = Boston)
summary(lm.fit9)
lm.fit10 = lm(crim~black, data = Boston)
summary(lm.fit10)
lm.fit11 = lm(crim~lstat, data = Boston)
summary(lm.fit11)
lm.fit12 = lm(crim~medv, data = Boston)
summary(lm.fit12)
plot(zn, crim)
plot(medv, crim)
```
(a) most of the predictors are highly significant with one exception, being by the Charles river is not significant. both of the plots show as zn or medv increase, crim decreases substantially.
```{r}
lm.fitm = lm(crim~., data = Boston)
summary(lm.fitm)
```
(b)Many of the predictors are now insignificant. Distance from employment centers has a highly negative association with crime rate, and accessibility to highways has a highly positive association with crime rate. We can reject the null hypothesis for zn, nox, dis, rad, black, lstat, and medv at the 10% level. 
```{r}
lrcoef = c(coefficients(lm.fit)[2], coefficients(lm.fit1)[2],
           coefficients(lm.fit2)[2], coefficients(lm.fit3)[2],
           coefficients(lm.fit4)[2], coefficients(lm.fit5)[2],
           coefficients(lm.fit6)[2], coefficients(lm.fit7)[2],
           coefficients(lm.fit8)[2], coefficients(lm.fit9)[2],
           coefficients(lm.fit10)[2], coefficients(lm.fit11)[2],
           coefficients(lm.fit12)[2])
mlrcoef = coefficients(lm.fitm)[2:14]
plot(lrcoef, mlrcoef)
```
(c) Most of the simple and multiple regression coefficients are close to 0 which means they are similar to each other. nox is the only outlier with the simple linear coefficient value being 31 and the multiple linear regression coefficent value being -10.  
```{r}
lm.fitq = lm(crim~poly(zn, 3), data = Boston)
summary(lm.fitq)
anova(lm.fit, lm.fitq)
lm.fitq1 = lm(crim~poly(indus, 3), data = Boston)
summary(lm.fitq1)
anova(lm.fit1, lm.fitq1)
lm.fitq3 = lm(crim~poly(nox, 3), data = Boston)
summary(lm.fitq3)
anova(lm.fit3, lm.fitq3)
lm.fitq4 = lm(crim~poly(rm, 3), data = Boston)
summary(lm.fitq4)
anova(lm.fit4, lm.fitq4)
lm.fitq5 = lm(crim~poly(age, 3), data = Boston)
summary(lm.fitq5)
anova(lm.fit5, lm.fitq5)
lm.fitq6 = lm(crim~poly(dis, 3), data = Boston)
summary(lm.fitq6)
anova(lm.fit6, lm.fitq6)
lm.fitq7 = lm(crim~poly(rad, 3), data = Boston)
summary(lm.fitq7)
anova(lm.fit7, lm.fitq7)
lm.fitq8 = lm(crim~poly(tax, 3), data = Boston)
summary(lm.fitq8)
anova(lm.fit8, lm.fitq8)
lm.fitq9 = lm(crim~poly(ptratio, 3), data = Boston)
summary(lm.fitq9)
anova(lm.fit9, lm.fitq9)
lm.fitq10 = lm(crim~poly(black, 3), data = Boston)
summary(lm.fitq10)
anova(lm.fit10, lm.fitq10)
lm.fitq11 = lm(crim~poly(lstat, 3), data = Boston)
summary(lm.fitq11)
anova(lm.fit11, lm.fitq11)
lm.fitq12 = lm(crim~poly(medv, 3), data = Boston)
summary(lm.fitq12)
anova(lm.fit12, lm.fitq12)
```
(d) there is evidence of non-linear association between the predictors and the response. the only predictors that do not have a significant f stat from the anova test is the binary variables black and indus.

Chapter 6 Question 9
```{r}
set.seed(1)
library(ISLR)
sample = sample(nrow(College),nrow(College)*0.75)
train = College[sample,]
test = College[-sample,]
```
(a) the above commands answer part (a)
```{r}
lm.fit = lm(Apps~., data = train)
lm.pred = predict(lm.fit,test)
cat("linear reg test MSE =",mean((test$Apps-lm.pred)^2))
```
(b) the above commands answer part (b)
```{r, warning=FALSE}
library(glmnet)
grid = 10^seq(10, -2, length = 100)
train.m = model.matrix(Apps~., data = train)
test.m = model.matrix(Apps~., data = test)
ridge.mod = glmnet(train.m, train$Apps, alpha = 0, lambda = grid)
cv.ridge = cv.glmnet(train.m, train$Apps, alpha = 0, lambda = grid)
bestlam.r = cv.ridge$lambda.min
ridge.pred = predict(ridge.mod, s = bestlam.r, newx = test.m)
cat("ridge test MSE =",mean((test$Apps - ridge.pred)^2) )
```
(c) the above commands answer part (c)
```{r}
lasso.mod = glmnet(train.m, train$Apps, alpha = 1, lambda = grid)
cv.lasso = cv.glmnet(train.m, train$Apps, alpha = 1, lambda = grid)
bestlam.l = cv.lasso$lambda.min
lasso.pred = predict(lasso.mod, s = bestlam.l, newx = test.m)
cat("lasso test MSE =",mean((test$Apps - lasso.pred)^2))
```
(d) the above commands answer part (d)
```{r, warning=FALSE}
library(pls)
pcr.fit = pcr(Apps~., data = train, scale = TRUE, validation = "CV")
validationplot(pcr.fit, val.type = "MSEP")
pcr.pred = predict(pcr.fit, test, ncomp = 17)
cat("PCR test MSE =",mean((test$Apps - pcr.pred)^2))
```
(e) the above commands answer part (e)
```{r}
pls.fit = plsr(Apps~., data = train, scale = TRUE, validation = "CV")
validationplot(pls.fit, val.type = "MSEP")
pls.pred = predict(pls.fit, test, ncomp = 9)
cat("PLS test MSE =",mean((test$Apps - pls.pred)^2))
```
(f) the above commands answer part (f)
```{r}
plot(lasso.mod)
bestlam.l
predict(cv.lasso, s = bestlam.l, type = "coefficients")
test.average = mean(test[, "Apps"])
lasso.r2 = 1 - mean((test[, "Apps"] - lasso.pred)^2)/mean((test[, "Apps"] - test.average)^2)
print(paste0("lasso r^2 = ",lasso.r2))
```
(g) PLS has the smallest CV MSE at 1376378. lasso, MLR, ridge, and PCR having slighly higher MSE's than PLS. Using lasso, the r^2 is .909 which means that we can explain 91% of the variation in applications recieved with the variation in the independent variables. there is not much difference between the the test error results of the five approaches. 

Chapter 6 Question 11
```{r}
library(MASS)
library(glmnet)
library(ISLR)
library(leaps)
library(pls)
summary(Boston)
#?Boston
```
```{r}
regfit.full = regsubsets(crim~., Boston, nvmax = 13)
reg.summary = summary(regfit.full)
which.max(reg.summary$adjr2)
which.min(reg.summary$cp)
which.min(reg.summary$bic)
coef(regfit.full, 9)
coef(regfit.full, 8)
coef(regfit.full, 3)
```
```{r}
set.seed(1)
sample = sample(nrow(Boston),nrow(Boston)*0.5)
train = Boston[sample,]
test = Boston[-sample,]
grid = 10^seq(10, -2, length = 100)
train.m = model.matrix(crim~., data = train)
test.m = model.matrix(crim~., data = test)
```
```{r}
ridge.mod = glmnet(train.m, train$crim, alpha = 0, lambda = grid)
cv.ridge = cv.glmnet(train.m, train$crim, alpha = 0, lambda = grid)
bestlam.r = cv.ridge$lambda.min
ridge.pred = predict(ridge.mod, s = bestlam.r, newx = test.m)
cat("ridge test RMSE =",sqrt(mean((test$crim - ridge.pred)^2)))
```
```{r, warning=FALSE}
library(plotmo)
lasso.mod = glmnet(train.m, train$crim, alpha = 1, lambda = grid)
cv.lasso = cv.glmnet(train.m, train$crim, alpha = 1, lambda = grid)
bestlam.l = cv.lasso$lambda.min
bestlam.l
lasso.pred = predict(lasso.mod, s = bestlam.l, newx = test.m)
cat("lasso test RMSE =",sqrt(mean((test$crim - lasso.pred)^2)))
x = model.matrix(crim~., Boston)[,-1]
out = glmnet(x, Boston$crim, alpha = 1, lambda = grid)
lasso.coef = predict(out, type = "coefficients", s = bestlam.l)
lasso.coef
plot(lasso.mod)
plot_glmnet(lasso.mod, label=5)
```
```{r}
pcr.fit = pcr(crim~., data = Boston, subset = sample, scale = TRUE, validation = "CV")
validationplot(pcr.fit, val.type = "MSEP")
summary(pcr.fit)
```
(a) Using best subset selection, the adjusted R^2 estimated that a 9 predictor model was optimal, the AIC estimated that a 8 predictor model is optimal, and the BIC estimated that a 3 variable model is optimal. Using CV ridge, the test MSE was 6.41. Using CV lasso, the test MSE was 6.39 and variables age and tax coefficients were pushed to 0. Using CV PCR, the lowest adjusted MSE was 6.89 with the 13 variable model but the 4 variable model was close behind with 6.96.

(b) I would choose the lasso model with lambda = .0705. I choose this model since it had the lowest test MSE compared to the other models. Even though the ridge model had a very similar test MSE, lasso is more interpretable than ridge is. 

(c) My choosen model does not include all of the independent variables in the data set. one reason for this is becuase the best model based on test MSE excluded 2 of the variables. Another reason is that best subset selection also reduced the number of variables from 13 to somewhere between 3 and 9 depending on which method was used. Comparing lasso to best subset selection, the adjusted R^2 and AIC had the same predictors as lasso and lasso included additional variables. Overall, the main reason for reducing the number of independent variables was to increase iterpretability and eliminate irrelevant variables that could lead to unnecessary complexity in our model.

Chapter 4 Question 10
```{r}
library(ISLR)
summary(Weekly)
attach(Weekly)
pairs(~., data = Weekly)
```
(a) most of the variables seem to be uncorrelated with eachother. The only trend that stands out is between Year and Volume with Volume increasing gradually over the years.
```{r}
glm.fit = glm(Direction~Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly, family = binomial)
summary(glm.fit)
table(Direction)
```
(b) Only the second lag is significant at the 5% level.
```{r}
glm.probs = predict(glm.fit, type = "response")
glm.probs[1:10]
contrasts(Direction)
glm.predi = rep("Down", 1089)
glm.predi[glm.probs > .5] = "Up"
table(glm.predi, Direction)
(557 + 54)/1089
mean(glm.predi == Direction)
```
(c) the diagonal elements of the confusion matrix indicate correct predictions, and the off-diagonals represent incorrect predictions. So the logistics regression correctly predicted the direction of the market 56.1% of the time.
```{r}
set.seed(1)
train = (Year < 2009)
Weekly.2009.10 = Weekly[!train,]
dim(Weekly.2009.10)
Direction.2009.10 = Direction[!train]
glm.fit1 = glm(Direction~ Lag2, data = Weekly, family = binomial, subset = train)
glm.probs = predict(glm.fit1, Weekly.2009.10, type = "response")
glm.pred = rep("Down", 104)
glm.pred[glm.probs > .5] = "Up"
table(glm.pred, Direction.2009.10)
print(paste0( "% of correct prediction = ",mean(glm.pred == Direction.2009.10)))
```
(d) the above commands answer part (d)
```{r}
library(class)
set.seed(1)
train.X = as.matrix(Lag2[train])
test.X = as.matrix(Lag2[!train])
train.Direction = Direction[train]
set.seed(1)
knn.pred = knn(train.X, test.X, train.Direction, k = 1)
table(knn.pred, Direction.2009.10)
mean(knn.pred == Direction.2009.10)
```
(g) the above commands answer (g)
(h) Logistics regression appears to provide the best results compared to KNN since it correcly predicted the outcome 62.5% of the time compared to KNN which correctly predicted the outcome 50% of the time.
```{r}
glm.fit2 = glm(Direction~ Lag2+Lag2^2, data = Weekly, family = binomial, subset = train)
glm.probs = predict(glm.fit2, Weekly.2009.10, type = "response")
glm.pred = rep("Down", 104)
glm.pred[glm.probs > .5] = "Up"
table(glm.pred, Direction.2009.10)
print(paste0( "% of correct prediction = ",mean(glm.pred == Direction.2009.10)))
```
```{r}
glm.fit2 = glm(Direction~ Lag1:Lag2, data = Weekly, family = binomial, subset = train)
glm.probs = predict(glm.fit2, Weekly.2009.10, type = "response")
glm.pred = rep("Down", 104)
glm.pred[glm.probs > .5] = "Up"
table(glm.pred, Direction.2009.10)
print(paste0( "% of correct prediction = ",mean(glm.pred == Direction.2009.10)))
```
```{r}
glm.fit2 = glm(Direction~. -Today, data = Weekly, family = binomial, subset = train)
glm.probs = predict(glm.fit2, Weekly.2009.10, type = "response")
glm.pred = rep("Down", 104)
glm.pred[glm.probs > .5] = "Up"
table(glm.pred, Direction.2009.10)
print(paste0( "% of correct prediction = ",mean(glm.pred == Direction.2009.10)))
```
```{r}
cat("k = 5\n")
knn.pred = knn(train.X, test.X, train.Direction, k = 5)
table(knn.pred, Direction.2009.10)
mean(knn.pred == Direction.2009.10)

cat("k = 10\n")
knn.pred = knn(train.X, test.X, train.Direction, k = 10)
table(knn.pred, Direction.2009.10)
mean(knn.pred == Direction.2009.10)

cat("k = 20\n")
knn.pred = knn(train.X, test.X, train.Direction, k = 20)
table(knn.pred, Direction.2009.10)
mean(knn.pred == Direction.2009.10)

cat("k = 50\n")
knn.pred = knn(train.X, test.X, train.Direction, k = 50)
table(knn.pred, Direction.2009.10)
mean(knn.pred == Direction.2009.10)

cat("k = 100\n")
knn.pred = knn(train.X, test.X, train.Direction, k = 100)
table(knn.pred, Direction.2009.10)
mean(knn.pred == Direction.2009.10)
```
(i) experimenting with log and KNN models, the log model with just an interaction term between Lag1 and Lag2 correctly predicted the outcome 58.65% of the time. The log model just predicted UP 98% of the time. The best KNN model was k = 20 which correctly predicted the outcome 59.61% of the time. KNN predicted UP 59% of the time. So KNN with k = 20 would be the best model other than the log polynomial model. These two models had the highest correct predicted probabilities other than the log model with a sqaured term of Lag2. The polynomial log model predicted the correct outcome 62.5% of the time. 

Chapter 8, Question 8
```{r, warning=FALSE, message=FALSE}
library(ISLR)
library(tree)
attach(Carseats)
#?Carseats
set.seed(1)
train = sample(1:nrow(Carseats), 200)
test = (-train)
Carseats.test = Carseats[-train,]
Sales.test = Sales[-train]
```
(a) the above commands answer part (a)
```{r}
tree.carseats = tree(Sales~., Carseats, subset = train)
tree.pred = predict(tree.carseats, Carseats.test)
y = Carseats$Sales
y.test = y[test]
cat("reg tree test MSE =",mean((tree.pred - y.test)^2))
plot(tree.carseats)
text(tree.carseats, pretty = 0)
```
(b) the test MSE is 4.922. The tree indicates that shelve location is the most important variable with bad and medium shelve location reducing the sales of the carseats compared to a good shelve location. Price is the second most important variable. 
```{r}
set.seed(1)
cv.carseats = cv.tree(tree.carseats, FUN = prune.tree)
cv.carseats
prune.carseats = prune.tree(tree.carseats, best = 18)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
tree.cv.pred = predict(prune.carseats, Carseats.test)
mean((tree.cv.pred - y.test)^2)
```
(c) pruning the tree does not improve test MSE since the pruned MSE was 4.922 which is the same as a regression tree. This is becuase the best dev used all the predictors. 
```{r}
library(randomForest)
set.seed(1)
dim(Carseats)
bag.Carseats = randomForest(Sales~., data = Carseats[train,], mtry = 10, ntree = 500, importance = TRUE)
bag.Carseats
yhat.bag = predict(bag.Carseats, Carseats[-train,])
mean((Carseats.test$Sales - yhat.bag)^2)
importance(bag.Carseats)
```
(d) Bagging reduced the MSE to 2.605, the most important variables are price and shelve location.
```{r}
rforest.Carseats = randomForest(Sales~., data = Carseats[train,], ntree = 500, importance = TRUE)
rforest.Carseats
yhat.forest = predict(rforest.Carseats, Carseats[-train,])
mean((Carseats.test$Sales - yhat.forest)^2)
importance(rforest.Carseats)
```
(e) the test MSE for random forest was 3.054, the most important variables were  price, shelve location, and age. The error rate for RF was larger than bagging. the only difference between the two is that bagging allows all the variables to  be considered at each split where RF only considers p/3 variables. In this case, allowing all variables to be considered reduced the test MSE cosiderably. 

Chapter 8, Question 11
```{r}
library(gbm)
attach(Caravan)
#?Caravan
set.seed(1)
Caravan$Purchase = ifelse(Caravan$Purchase == "Yes", 1, 0)
train = sample(1:nrow(Caravan), 1000)
test = (-train)
Caravan.test = Caravan[-train,]
Caravan.train = Caravan[train,]
Purchase.test = Purchase[-train]
```
(a) the above commands answer (a)
```{r}
boost.caravan = gbm(Purchase~., data = Caravan.train, distribution = "gaussian", n.trees = 1000, interaction.depth = 4, shrinkage = 0.01, verbose = F)
summary(boost.caravan)
```
(b) The most important variables are PPERSAUT, PBRAND, and MOSTYPE.
```{r}
yhat.boost = predict(boost.caravan, Caravan.test, n.trees = 1000, type = "response")
boost.pred = ifelse(yhat.boost > 0.2, 1, 0)
table(Caravan.test$Purchase, boost.pred)
59/(59 + 297)
```
```{r, warning=FALSE}
log.caravan = glm(Purchase~., data = Caravan.train, family = "binomial")
log.prob = predict(log.caravan, Caravan.test, type = "response")
log.pred = ifelse(log.prob > 0.2, 1, 0)
table(Caravan.test$Purchase, log.pred)
52/(52+319)
```
(c) 16.6% of the people predicted to make a purchase actually made a purchase using a boosted model. 14.0% of the people predicted to make a puchase actually made a puchase using a log model.  


Problem 1, Beauty Pays!
```{r}
BeautyData <- read.csv("C:/Users/tsblo/Predictive modeling/Take_Home_Exam/BeautyData.csv")
summary(BeautyData)
```
```{r}
lm.fit = lm(CourseEvals~.*BeautyScore, data = BeautyData)
summary(lm.fit)
pairs(BeautyData)
```
```{r}
tree.beauty = tree(CourseEvals~., data = BeautyData)
plot(tree.beauty)
text(tree.beauty, pretty = 0)
```
1. from both linear regression and regression tree, the three most important variables are BeautyScore, gender, and lower division class. Increasing the attractiveness of the professor by 1 point, is predicted to increase the evaluation score by .31 holding the other variables constant. Being a female professor and teacher a lower division class is predicted to reduce the evaluation score. not being a native english speaking professor also reduced the predicted evaluation score. Non of the other predictors are significant at or below the 1% level. The regression tree only uses BeautyScore, female, and lower variables to predict evalutation score. None of the interaction terms with Beauty were significant. 
First, I was surpised by female professors getting lower scores than males. I was also suprised that none of the interaction terms were significant especially between female and Beauty. 

2. What the professor means is that we cannot say with certainty that Beauty has a causal effect on evaluation score. This is becuase the BeautyScore could be associated with other variables that are not in the dataset or the regression. This is called ommitted variable bias. becuase there are variables that effect Beauty, we cannot accuratly predict evaluations score. More attractive people may be more productive and hence are better teachers. Or productivity could have no correlation with attractiveness. But without including some productivity variable in the regression, we cannot say that students are discriminating against professor becuase of their looks or their teaching methods. 

Problem 2, Housing Price Structure
```{r}
MidCity <- read.csv("C:/Users/tsblo/Predictive modeling/Take_Home_Exam/MidCity.csv")
summary(MidCity)
```
```{r}
MidCity$Hood_2 = ifelse(MidCity$Nbhd == 2, 1, 0)
MidCity$Hood_3 = ifelse(MidCity$Nbhd == 3, 1, 0)
lm.fit = lm(Price~. +Hood_3:Brick-Nbhd, data = MidCity)
summary(lm.fit)
pairs(MidCity)
```
1. There is a premium for brick houses holding all else equal. if a house is made of brick, the price is predicted to increase by $13,839 over a house not made of brick holding all the other variables equal. 

2. yes, there is a premium on houses in neihborhood three. Looking at the dummy variable, Hood_3, a house in neighborhood 3 is predicted to have a price $17,086 higher than a house in neighborhood 1 holding all the other variables constant.

3. yes, there is a premium on brick houses in neighborhood 3. looking at th interaction term, a brick house in neighborhood 3 is predicted to have a price $10,192 higher than a non brick house in neighborhood 3 holding all else constant. One thing to note is that the interaction term is only significant at the 5% level so its signifigance is less compared to the past two questions.

```{r, warning=FALSE, message=FALSE}
MidCity$Hood_1and2 = ifelse(MidCity$Nbhd == 1 & MidCity$Nbhd == 2, 1, 0)
dim(MidCity)
attach(MidCity)
set.seed(1)
train = sample(128, 60)
test = (-train)

lm.fit = lm(Price~.+Hood_3:Brick-Nbhd-Hood_2, data = MidCity)
summary(lm.fit)

lm.fit = lm(Price~.+Hood_3:Brick-Nbhd-Hood_2, data = MidCity, subset = train)
print(paste0( "test MSE for OLS combining Nhbh 1 & 2 = ",mean((Price - predict(lm.fit, MidCity))[-train]^2)))

lm.fit = lm(Price~.+Hood_3:Brick-Nbhd-Hood_1and2, data = MidCity, subset = train)
print(paste0( "test MSE for OLS with seperate Nhbhs = ",mean((Price - predict(lm.fit, MidCity))[-train]^2)))
```
4. yes, you could combine neighborhoods 1 and 2 into a single binary variable, and as my validation set approach shows, the test MSE for combining the neighborhoods has a slightly lower test MSE than with the seperated neighborhoods but the test MSE's are quite similar. This is becuase both neighborhood 1 and 2 have very similar characteristics. So combining the variables does not decrease the predictive accuracy of OLS.

Problem 3, what causes what?
1. you cant just regress crime rate on number of cops becuase that will only show you the correlation between those two variables and will not answer the question: does number of cops have a causal effect on crime rate? This is becuase there is omitted variable bias, or variables that are not included in the regression that effect number of cops and crime rate. since these variables effect number of cops, they effect crime rate and the regression cannot dicipher which variables truly effect crime rate. 

2. The UPENN researchers used an ingenius idea. they used an instrumental variable, high alert. they replaced number of cops in the regression with high alert. High alert is associated with more cops on the streets, so there is a high correlation between the two. But the researchers are assuming that high alert is not associated with, not correlated with, any other omitted variable that are also correlated with crime levels. this gets around the problem of omitted variable bias, and if their assumptions are correct, the variable high alert can determine if there is a causal effect of number of cops on crime. Table 2 shows that in the simple regression of crime rate on high alert, being on high alert is predicted to decrease crime rate by 7.316 percent and is significant at the 5% level. Table 2 also shows the multi linear regression of crime rate on high alert and midday ridership. being on high alert is predicted to decrease crime by 6.046 percent holding midday ridership fixed and is significant at the 5% level. 

3. Metro ridership could have gone down during a high risk day which could reduce crime since there are less people on the streets. if this were the case, the high risk variable would be correlated with metro ridership and would therefore not be an instrumental variable.

4. this is a multi linear regression of crime rate on high alert + district 1 interaction variable, high alert + other districts interaction variable, and midday ridership variable. the coefficent of high alert + district 1 can be interpreted as if washington has a high alert day, crime rate of district 1 is predicted to decrease by 2.621 percent at the 1% signifigance level compared to other districts. the coefficent for high alert + other districts is not significant. midday ridership variable is significant and positive which means ridership actually goes up during high risk days. If high alert is truly an instrumental variable for number of cops, this means that increasing the number of cops on the streets causes crime to decrease in district 1 but not in other districts. So without interaction terms like in table 2, it could be interpreted as increasing the number of cops decreases all crime in washington but in actuality, as table 4 shows, increasing the number of cops only decreases crime in district 1 and not any of the other districts.  

Problem 4, BART (WANING! MOST OF THIS CODE IS CARLOS'S, I JUST USED IT TO GET THE RMSE OF RANDOM FOREST AND BOOSTING)
```{r}
library(randomForest)
ca <- read.csv("CAhousing.csv",header=TRUE)
ca$AveBedrms <- ca$totalBedrooms/ca$households
ca$AveRooms <- ca$totalRooms/ca$households
ca$AveOccupancy <- ca$population/ca$households
logMedVal <- log(ca$medianHouseValue)
ca <- ca[,-c(4,5,9)] # lose lmedval and the room totals
ca$logMedVal = logMedVal

#--------------------------------------------------
#train, val, test
set.seed(99)
n=nrow(ca)
n1=floor(n/2)
n2=floor(n/4)
n3=n-n1-n2
ii = sample(1:n,n)
catrain=ca[ii[1:n1],]
caval = ca[ii[n1+1:n2],]
catest = ca[ii[n1+n2+1:n3],]
```
```{r}
set.seed(1)
catrainval = rbind(catrain,caval)
finrf = randomForest(logMedVal~.,data=catrainval,mtry=3,ntree=500)
finrfpred=predict(finrf,newdata=catest)

finrfrmse = sqrt(sum((catest$logMedVal-finrfpred)^2)/nrow(catest))
cat('finrfrmse: ',finrfrmse,'\n')

```
random forest regression, RMSE = .237
```{r}
library(gbm)

set.seed(1)
catrainval = rbind(catrain,caval)
ntrees=5000
finb = gbm(logMedVal~.,data=catrainval,distribution='gaussian',
              interaction.depth=4,n.trees=ntrees,shrinkage=.2)
finbpred=predict(finb,newdata=catest,n.trees=ntrees)


finbrmse = sqrt(sum((catest$logMedVal-finbpred)^2)/nrow(catest))
cat('finbrmse: ',finbrmse,'\n')

```
boosting, RMSE = .237
```{r}
library(MASS)
x = ca[,1:9] 
y = ca$logMedVal 
head(cbind(x,y))

library(BART) #BART package
set.seed(1) #MCMC, so set the seed
nd=200 # number of kept draws
burn=50 # number of burn in draws
bf = wbart(x,y,nskip=burn,ndpost=nd)

lmf = lm(y~.,data.frame(x,y))
fitmat = cbind(y,bf$yhat.train.mean,lmf$fitted.values)
colnames(fitmat)=c("y","BART","Linear")
cor(fitmat)

n=length(y) #total sample size
set.seed(1) #
ii = sample(1:n,floor(.75*n)) # indices for train data, 75% of data
xtrain=x[ii,]; ytrain=y[ii] # training data
xtest=x[-ii,]; ytest=y[-ii] # test data
cat("train sample size is ",length(ytrain)," and test sample size is ",length(ytest),"\n")

set.seed(1)
bf_train = wbart(xtrain,ytrain)
yhat = predict(bf_train,as.matrix(xtest))

yhat.mean = apply(yhat,2,mean) 

finbrmse = sqrt(sum((ytest-yhat.mean)^2)/length(ytest))
cat('BART finbrmse: ',finbrmse,'\n')
```
Bart does not do as well as random forest or boosting looking at the validation RMSE, both RF and boosting have RMSE's of .237 where as Bart has RMSE of .244.

Problem 5, Neural Nets
```{r, message=FALSE}
attach(Boston)
library(nnet)
dim(Boston)
```
```{r}
set.seed(1)
maxs = as.numeric(apply(Boston, 2, max))
mins = as.numeric(apply(Boston, 2, min))
Boston = scale(Boston, center = mins, scale = maxs - mins)
n = nrow(Boston)
ii = sample(1:n, n)
n1=floor(n/2)
n2=floor(n/4)
n3=n-n1-n2
ii = sample(1:n,n)
Btrain= Boston[ii[1:n1],]
Bval = Boston[ii[n1+1:n2],]
Btest = Boston[ii[n1+n2+1:n3],]
```
```{r}
set.seed(1)
Btrainval = rbind(Btrain, Bval)
decayList = c(.00001, .0001, .001, .01, .1, .3, .5, .8)
for (decay in decayList){
  sizeList = c(1, 3, 5, 10, 25, 50)
  for (size in sizeList){
    Bnn = nnet(crim~.,data = Btrainval,size=size,decay=decay,linout=T, trace = FALSE)
    pBnn = predict(Bnn, newdata = Btest)
    cv.rmse = sqrt(sum((Btest[, "crim"] -pBnn)^2)/nrow(Btest))
    cat("rmse:", cv.rmse, "decay =", decay, "size =", size, "\n")
}}
```
after cross validating for a few decay and size parameters, found that the best decay, size parameter pair was .00001, 50 with an RMSE of .087. decay = .01, size = 10 was the second best with an RMSE of .088. Just like in the slides, it is probably more important to CV for the decay parameter and just have a large size as default. 


Problem 6: final project

My team was having a hard time finding a data set that we could easily create models for ie most of the data sets we liked had a lot of prepossesing work which we were not fimilair with. I found a data set that was clean and we chose to select it for our project. I created dummy variables for some of the categorical variables. I ran linear regression models with and without interaction terms and used a validation set approach to calculate test MSE. I ran cross validation lasso and ridge models and made interpretable graphs for each for our presentation. I ran basic regression tree, bagging, boosting, and randomforest models all using validation approach to calculate MSE. I ran cross validation boosting. I also helped structure the power point presentation and created some of the slides.





