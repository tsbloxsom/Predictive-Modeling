---
title: "Star_Project"
output: html_document
---

```{r}
project_star_ps1 <- read_dta("C:/Users/tsblo/Predictive modeling/R_Carlos_project/project_star_ps1.dta")
project_star_ps1 = na.omit(project_star_ps1)
```
loading in the data set and taking out any ommited variable rows
```{r}
project_star_ps1$male = ifelse(project_star_ps1$ssex == 1, 1, 0)
project_star_ps1$white = ifelse(project_star_ps1$srace == 1, 1, 0)
project_star_ps1$black = ifelse(project_star_ps1$srace == 2, 1, 0)
project_star_ps1$free_lunch = ifelse(project_star_ps1$sesk == 1, 1, 0)
project_star_ps1$small_class = ifelse(project_star_ps1$cltypek == 1, 1, 0)
project_star_ps1$white_teacher = ifelse(project_star_ps1$tracek == 1, 1, 0)
project_star_ps1$black_teacher = ifelse(project_star_ps1$tracek == 2, 1, 0)
```
creating dummy vraibles for categorical varibales with wierd codings
```{r}
star_data = data.frame("test_score" = project_star_ps1$tcombssk, "male" = project_star_ps1$male, "white" = project_star_ps1$white, "black" = project_star_ps1$black, "free_lunch" = project_star_ps1$free_lunch,"small_class" = project_star_ps1$small_class, "teacher_degree" = project_star_ps1$hdegk, "years_teaching" = project_star_ps1$totexpk, "School_type" = project_star_ps1$schtypek, "white_teacher" = project_star_ps1$white_teacher, "black_teacher" = project_star_ps1$black_teacher)
```
creating a new data frame with the variables that are going to be used in predicting test score
```{r}
dim(star_data)
star_data$scaled_years_teaching = scale(star_data$years_teaching)
star_data = subset(star_data, select = -c(years_teaching))
summary(star_data)
```
need to see how many rows there are for sampling method, also scaling numeric variable
```{r}
set.seed(1)
train = sample(5726, 4000)
test = (-train)
x = model.matrix(test_score~., star_data)[,2:11]
y = star_data$test_score
y.test = test_score[test]
star.test = star_data[-train,]
#pairs(star_data)
```
create a train and test set, does not seem like there is any nonlinearity

```{r}
regfit.full = regsubsets (test_score∼.,star_data)
summary(regfit.full)
```
#Using regsubsets function to perform best subset selection on whole data

```{r}
regfit.full = regsubsets (test_score∼.,star_data,nvmax=10)
reg.summary = summary(regfit.full)
names(reg.summary)
reg.summary$rsq
```
#Fitting a 9 variable model using best subset selection on whole data
#R^2 statistic increases monotonically as more variables are included

```{r}
par(mfrow=c(2,2))
plot(reg.summary$rss ,xlab="Number of Variables ",ylab="RSS",
type="l")
```
#Plotting RSS for all the models

```{r}
plot(reg.summary$adjr2 ,xlab="Number of Variables ", ylab="Adjusted RSq",type="l")
which.max(reg.summary$adjr2)
points(7,reg.summary$adjr2[11], col="red",cex=2,pch =20)
```
#Plotting adjusted R^2 for all the models

```{r}
plot(reg.summary$cp,xlab="Number of Variables ",ylab="Cp",type="l")
which.min(reg.summary$cp )
points (6,reg.summary$cp [6], col ="red",cex=2,pch =20)
which.min(reg.summary$bic )
plot(reg.summary$bic ,xlab="Number of Variables ",ylab="BIC",type="l")
points (6,reg.summary$bic [6],col="red",cex=2,pch =20)
```
#Plotting Cp and BIC for all the models

```{r}
regfit.fwd=regsubsets (test_score∼.,data=star_data , nvmax=10,
method ="forward")
summary (regfit.fwd)
```
#Forward stepwise selection on whole data

```{r}
regfit.bwd=regsubsets (test_score∼.,data=star_data , nvmax=10,
method ="backward")
summary (regfit.bwd)
```
#Backwise stepwise selection on whole data

```{r}
regfit.best=regsubsets (test_score∼.,data=star_data[train ,],nvmax=10)
summary(regfit.best)
```
#Applying regsubsets to training data to perform best subset selection

```{r}
test.mat=model.matrix(test_score∼.,data=star_data [test ,])
```
#Computing the validation set error for the best model of each model size

```{r}
val.errors =rep(NA ,9)
for(i in 1:9){
  coefi=coef(regfit.best ,id=i)
  pred=test.mat[,names(coefi)]%*%coefi
  val.errors[i]=mean(( star_data$test_score[test]-pred)^2)
  }
val.errors
which.min(val.errors)
```
#Computing MSE

#Best model is the one with 7 variables

```{r}
coef(regfit.best ,7)
```
#Coeficient estimates using training data set
```{r}
coef(regfit.full,7)
```
#Coeficient estimates using full data set

```{r}
lm.fit = lm(test_score~.*free_lunch*small_class, data = star_data, subset = train)
summary(lm.fit)
attach(star_data)
print(paste0( "test MSE for OLS with interaction terms = ",mean((test_score - predict(lm.fit, star_data))[-train]^2)))
```
#Trying linear regression with interaction terms

```{r}
regfit.full = regsubsets(test_score∼.*free_lunch*small_class, star_data[train,], nvmax=10)
#reg.summary = 
summary(regfit.full)
#names(reg.summary)
#reg.summary$rsq
test.mat=model.matrix(test_score∼.*free_lunch*small_class,data=star_data [test ,])
```


```{r}
val.errors =rep(NA ,9)
for(i in 1:9){
  coefi=coef(regfit.full ,id=i)
  pred=test.mat[,names(coefi)]%*%coefi
  val.errors[i]=mean(( star_data$test_score[test]-pred)^2)
  }
val.errors
which.min(val.errors)
```

```{r}
lm.fit = lm(test_score~., data = star_data, subset = train)
summary(lm.fit)
attach(star_data)
print(paste0( "test MSE for OLS = ",mean((test_score - predict(lm.fit, star_data))[-train]^2)))
mean((y - mean(star_data$test_score))^2)
```
linear regression 
```{r}
lm.fit = lm(test_score~.*free_lunch*small_class, data = star_data, subset = train)
summary(lm.fit)
attach(star_data)
print(paste0( "test MSE for OLS with interaction terms = ",mean((test_score - predict(lm.fit, star_data))[-train]^2)))
```
linear regression with interaction terms
```{r}
library(plotmo)
grid = 10^seq(10, -2, length = 100)
lasso.mod = glmnet(x[train,], y[train], alpha = 1, lambda = grid)
plot(lasso.mod, label=TRUE)
plot_glmnet(lasso.mod, label=5)
set.seed(1)
cv.out = cv.glmnet(x[train,], y[train], alpha = 1)
plot(cv.out)
bestlam = cv.out$lambda.min
lasso.pred = predict(lasso.mod, s = bestlam, newx = x[test,])
mean((lasso.pred - y.test)^2)
out = glmnet(x, y, alpha = 1, lambda = grid)
lasso.coef = predict(out, type = "coefficients", s = bestlam)
lasso.coef
```
lasso regression
```{r}
ridge.mod = glmnet(x[train,], y[train], alpha = 0, lambda = grid)
plot(ridge.mod)
plot_glmnet(ridge.mod, label=5)
set.seed(1)
cv.out = cv.glmnet(x[train,], y[train], alpha = 0)
plot(cv.out)
bestlam = cv.out$lambda.min
ridge.pred = predict(ridge.mod, s = bestlam, newx = x[test,])
mean((ridge.pred - y.test)^2)
```
ridge regression
```{r}
tree.star = tree(test_score~., star_data, subset = train)
tree.pred = predict(tree.star, star.test)
mean((tree.pred - y.test)^2)
plot(tree.star)
text(tree.star, pretty = 0)
summary(tree.star)
?tree()
```
tree regression
```{r}
bag.star = randomForest(test_score~., data = star_data[train,], ntree = 500, importance = TRUE)
bag.star
yhat.bag = predict(bag.star, star_data[-train,])
mean((star.test$test_score - yhat.bag)^2)
importance(bag.star)
```
tree bagging
```{r}
rforest.star = randomForest(test_score~., data = star_data[train,], mtry = 5, ntree = 500, importance = TRUE)
rforest.star
yhat.forest = predict(rforest.star, star_data[-train,])
mean((star.test$test_score - yhat.forest)^2)
importance(rforest.star)
```
random forest
```{r}
boost.star = gbm(test_score~., data = star_data[train,], distribution = "gaussian", n.trees = 5000, interaction.depth = 4)
summary(boost.star)
yhat.boost = predict(boost.star, newdata = star_data[-train,], n.trees = 5000)
mean((star.test$test_score - yhat.boost)^2)
#varImpPlot(boost.star)
```
boosting
```{r}
library(randomForest)
library(rpart)
set.seed(1)
train = sample(5726, 4000)
test = (-train)
x = model.matrix(test_score~., star_data)[,2:11]
y = star_data$test_score
y.test = star_data$test_score[test]
star.train = star_data[train,]
star.test = star_data[-train,]

#fit a big tree using rpart.control
big.tree = rpart(star_data$test_score~star_data$scaled_years_teaching+star_data$free_lunch+star_data$male+star_data$white+star_data$black+star_data$small_class+star_data$teacher_degree+star_data$School_type+star_data$white_teacher+star_data$black_teacher,method="anova",data= star.train,
                 control=rpart.control(minsplit=5,cp=.0005))
nbig = length(unique(big.tree$where))
cat('size of big tree: ',nbig,'\n')

oo=order(star.train$test_score)
bestcp=big.tree$cptable[which.min(big.tree$cptable[,"xerror"]),"CP"]
cat('bestcp: ',bestcp,'\n')
cpvec = c(.0157,bestcp,.004)
par(mfrow=c(3,2))
for(i in 1:3) {
  ptree = prune(big.tree,cp=cpvec[i])
  pfit = predict(ptree)
}

best.tree = prune(big.tree,cp=bestcp)


mse = mean((test_score - predict(best.tree, star_data))[-train]^2)
sqrt(mse)

```
```{r}
train = data.frame(star_data)
test = data.frame(star_data)

kcv = 10
n0 = round(n/kcv,0)

out_MSE = matrix(0,kcv,100)

used = NULL
set = 1:n

for(j in 1:kcv){

if(n0<length(set)){val = sample(set,n0)}
if(n0>=length(set)){val=set}

train_i = train[-val,]
test_i = test[val,]

for(i in 1:100){

near = kknn(test_score~.,train_i,test_i,k=i,kernel = "rectangular")
aux = mean((test_i[,2]-near$fitted)^2)

out_MSE[j,i] = aux
}

used = union(used,val)
set = (1:n)[-used]

cat(j,'\n')

}

mMSE = apply(out_MSE,2,mean)

plot(log(1/(1:100)),sqrt(mMSE),xlab="Complexity (log(1/k))",ylab="out-of-sample RMSE",col=4,lwd=2,type="l",cex.lab=1.2,main=paste("kfold(",kcv,")"))
best = which.min(mMSE)
text(log(1/best),sqrt(mMSE[best])+0.1,paste("k=",best),col=2,cex=1.2)
text(log(1/2),sqrt(mMSE[2])+0.3,paste("k=",2),col=2,cex=1.2)
text(log(1/100)+0.4,sqrt(mMSE[100]),paste("k=",100),col=2,cex=1.2)


```
```{r}
set.seed(1)
n = nrow(star_data)
ii = sample(1:n, n)
n1=floor(n/2)
n2=floor(n/4)
n3=n-n1-n2
ii = sample(1:n,n)
Strain= star_data[ii[1:n1],]
Sval = star_data[ii[n1+1:n2],]
Stest = star_data[ii[n1+n2+1:n3],]
```
```{r}
set.seed(1)
idv = c(4,10)
ntv = c(1000,5000)
lamv=c(.001,.2)
parmb = expand.grid(idv,ntv,lamv)
colnames(parmb) = c('tdepth','ntree','lam')
print(parmb)
nset = nrow(parmb)
olb = rep(0,nset)
ilb = rep(0,nset)
bfitv = vector('list',nset)
for(i in 1:nset) {
    cat('doing boost ',i,' out of ',nset,'\n')
   tempboost = gbm(test_score~.,data=Strain,distribution='gaussian',
                interaction.depth=parmb[i,1],n.trees=parmb[i,2],shrinkage=parmb[i,3])
   ifit = predict(tempboost,n.trees=parmb[i,2])
   ofit=predict(tempboost,newdata=Sval,n.trees=parmb[i,2])
   olb[i] = sum((Sval$test_score-ofit)^2)
   ilb[i] = sum((Strain$test_score-ifit)^2)
   bfitv[[i]]=tempboost
}
ilb = round(sqrt(ilb/nrow(Strain)),3); olb = round(sqrt(olb/nrow(Sval)),3)
#--------------------------------------------------
#print losses

print(cbind(parmb,olb,ilb))

#--------------------------------------------------
#write val preds
iib=which.min(olb)
theb = bfitv[[iib]] 
thebpred = predict(theb,newdata=Sval,n.trees=parmb[iib,2])
write(thebpred,file='thebpred-2.txt',ncol=1)

mse = mean((Sval$test_score -predict(theb,newdata=Sval,n.trees=parmb[iib,2]))^2)
sqrt(mse)

```
